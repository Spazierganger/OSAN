{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete_forward(self, logits): \n",
    "    self.samples = self.sample_gumbel_k(tf.shape(logits))\n",
    "    gumbel_sample = logits + self.samples\n",
    "    threshold = tf.expand_dims(tf.nn.top_k(gumbel_sample, self.k, sorted=True)[0][:,-1], -1)\n",
    "    y = tf.cast(tf.greater_equal(gumbel_sample, threshold), tf.float32)\n",
    "    return y\n",
    "    \n",
    "def sample_discrete_backward(self, logits):     \n",
    "    gumbel_sample = logits + self.samples\n",
    "    threshold = tf.expand_dims(tf.nn.top_k(gumbel_sample, self.k, sorted=True)[0][:,-1], -1)\n",
    "    y = tf.cast(tf.greater_equal(gumbel_sample, threshold), tf.float32)\n",
    "    return y\n",
    "\n",
    "@tf.custom_gradient\n",
    "def sample_graph(self, logits, adjacency_matrix):\n",
    "\n",
    "    # logits are the logits for the nodes of the graph with M nodes\n",
    "    # computed by an upstream neural network\n",
    "    # for the sake of simplicity, I assume logits to be of dimension (batch, M)\n",
    "    # this could be extended to (batch, M, d) later to sample d graphs\n",
    "    # the adjacency matrix is the input graph, it is a tensor of size (batch, M, M)\n",
    "    \n",
    "    # sample discretely with perturb and map; this is\n",
    "    z_train = self.sample_discrete_forward(logits)\n",
    "    # stop the g radients for z_train and the adjacency matrix\n",
    "    # we will return gradients only for the logits\n",
    "    z_train = tf.stop_gradient(z_train)\n",
    "    adjacency_matrix_train = tf.stop_gradient(adjacency_matrix)\n",
    "    # here we can now freely manipulate the adjacency matrix\n",
    "    # by setting entries to 0 which have not been selected \n",
    "    # we can do this any way we please\n",
    "    # we then return the adjacency matrix\n",
    "    \n",
    "    # compute the top-k discrete values\n",
    "    threshold = tf.expand_dims(tf.nn.top_k(logits, self.k, sorted=True)[0][:,-1], -1)\n",
    "    z_test = tf.cast(tf.greater_equal(logits, threshold), tf.float32)\n",
    "    z_test = tf.stop_gradient(z_test)\n",
    "    adjacency_matrix_test = tf.stop_gradient(adjacency_matrix)\n",
    "    # here we can now freely manipulate the adjacency matrix\n",
    "    # by setting entries to 0 which have not been selected \n",
    "    # we can do this any way we please\n",
    "    # we then return the adjacency matrix\n",
    "   \n",
    "    # at training time we sample, at test time we take the argmax\n",
    "    z_output = K.in_train_phase(adjacency_matrix_train, adjacency_matrix_test)\n",
    "        \n",
    "    def custom_grad(dy):\n",
    "        \n",
    "        # the tensor dy is of dimension (batch, M, M)\n",
    "        # we know have to aggregate the gradients (each for an edge in the graph)\n",
    "        # to obtain a gradient for each node\n",
    "        # here we take the sum of the gradients of row i to be the gradient of node i\n",
    "        # this can be shown to be reasonable for undirected graphs\n",
    "        # but other aggregation schemes are possible for directed graphs\n",
    "        node_gradients = tf.reduce_sum(dy, 1, keepdims=True)\n",
    "        # node_gradients has now size (batch, M, 1)\n",
    "        node_gradients = tf.reshape(node_gradients, [-1, M])\n",
    "        # now we can treat node_gradients as the gradients wrt the nodes\n",
    "        \n",
    "        # we perturb (implicit diff) and then resuse sample for perturb and MAP\n",
    "        map_dy = self.sample_discrete_backward(logits - (self._lambda*node_gradients))\n",
    "        # we now compute the gradients as the difference (I-MLE gradients)\n",
    "        grad = tf.math.subtract(z_train, map_dy)\n",
    "        # return the gradient            \n",
    "        return grad, adjacency_matrix\n",
    "\n",
    "    return z_output, custom_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
